# Mega-NeRF:Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs
# Abstract
我们使用神经辐射场(nerf)构建交互式3D环境, 利用无人机大规模视觉收集跨建筑甚至多个城市街区的数据。

与单一对象场景(传统上评估nerf的场景)相比，我们的尺度提出了多个挑战，包括

(1)需要在不同的光照条件下建模成千上万的图像，而每种图像只能捕捉场景的一小部分;

(2)不可避免的大模型容量，使得在单个GPU上训练不可行;

(3)快速渲染的重大挑战，将使交互式飞行。

为了解决这些挑战，我们首先分析大规模场景的可见性统计数据，激发一个稀疏网络结构，其中参数专门用于场景的不同区域。

我们介绍了一种简单的几何聚类算法用于数据并行，该算法将训练图像(或像素)划分为不同的NeRF子模块，这些子模块可以并行训练。

我们在现有数据集(Quad 6k和UrbanScene3D)以及我们自己的无人机镜头上评估了我们的方法，将训练速度提高了3倍，PSNR提高了12%。

我们还在Mega-NeRF的基础上评估了最近的NeRF快速渲染器，并介绍了一种利用时间相干性的新方法

我们的技术比传统的NeRF渲染速度提高了40倍，同时PSNR质量保持在0.8 db以内，超过了现有的快速渲染器的保真度。

## Introduction
神经渲染技术的最新进展导致了在照片逼真的新视图合成方面的重大进展，这是许多VR和AR的先决条件。

特别是神经辐射场(nerf)[24]已经引起了极大的关注，产生了广泛的后续工作，改进了原始方法的各个方面。

### Scale.
简单地说，我们的工作探索了nerf的可伸缩性。

现有的绝大多数方法都是探索单一对象场景，通常是在室内或从合成数据中捕获的。

据我们所知，Tanks and Temples[17]是NeRF评估使用的最大数据集，平均占地463平方米。

在这项工作中，我们scale nerf以从无人机镜头中捕捉和交互式可视化城市规模的环境，这些镜头比迄今为止的任何数据集都要大几个数量级，每个场景从150,000平方米到超过1,300,000平方米。

### Search and Rescue.
作为一个激励的用例，考虑搜索和救援，无人机提供了一种廉价的方法来快速测量一个区域，并优先考虑有限的第一反应资源(例如，地面团队部署)。

由于电池寿命和带宽限制了在实时[6]中捕获足够详细的镜头的能力，收集的镜头通常被重建为支持事后分析[42]的2D“鸟瞰图”

我们想象的未来，神经渲染将这种分析提升到3D，使响应团队能够检查现场，就像他们实时驾驶无人机一样，其详细程度远远超出了经典的结构从运动(SfM)所能实现的。

### Challenges
在这种背景下，我们会遇到多重挑战。

首先，像搜索和救援这样的应用程序是时间敏感的。

根据国家搜索和救援计划[1]，“受伤幸存者的预期寿命在最初24小时内减少80%，而未受伤幸存者的生存机会在前3天后迅速减少。”

因此，在几个小时内训练出一个适合我们的模型的能力是非常有价值的。

其次，由于我们的数据集比之前评估的数据集要大几个数量级(表1)，为了确保高视觉保真度，必须显著增加模型容量，进一步增加训练时间.

最后，尽管交互式渲染对于我们捕捉的规模的飞行和探索很重要，但现有的实时NeRF渲染器要么依赖于将输出预制表为有限分辨率的结构，这种结构的伸缩性很差，显著降低了渲染性能，要么需要过多的预处理时间。

### Mega-NeRF.
为了解决这些问题，我们提出了Mega-NeRF，一个用于大规模训练的框架3D场景，支持交互式飞行。

我们首先分析大规模场景的可见性统计数据，如表1所示。

![image](https://user-images.githubusercontent.com/48575896/226861670-3bac4849-bce0-4386-8f36-9239cbb1ab64.png)

由于只有一小部分训练图像从任何特定的场景点可见，我们引入了一个稀疏网络结构，其中参数专门用于场景的不同区域。

我们介绍了一个简单的几何聚类算法，该算法将训练图像(或像素)划分为不同的NeRF子模块，这些子模块可以并行训练.

我们进一步利用渲染时的空间局部性来实现即时可视化技术，该技术允许交互式飞行.

### Prior art.
我们使用“多个”NeRF子模块的方法受到DeRF[28]和KiloNeRF[29]最近工作的启发，它们使用类似的见解来加速预训练NeRF的渲染。

我们证明了模块化对于训练是至关重要的，特别是当与智能策略相结合时，通过几何聚类将训练数据“分片”到适当的模块中。

### Contributions.
我们建议重新表述NeRF架构，sparsifies层以空间感知的方式连接，促进训练和渲染时间的效率提高。

然后，我们调整训练过程以利用空间局部性并以完全并行的方式训练模型子权重，从而使训练速度提高了3倍，同时超过了现有方法的重建质量。

与此同时，我们评估了现有的快速渲染方法与我们训练的Mega-NeRF模型，并提出了一种利用时间相干性的新方法。

我们的技术只需要最少的预处理，避免了其他渲染器的有限分辨率下降，并保持了高水平的视觉保真度。

我们还提出了一个新的大规模数据集，包含从无人机拍摄的超过10万平方米的工业园区附近地形中收集的数千张高清图像。

## Approach
### Model Architecture
#### Spatial partitioning.
Mega-NeRF将场景分解为cells, 其质心为 ![image](https://user-images.githubusercontent.com/48575896/226876600-3a089f0a-0c39-4a1a-aa4a-c58428a96359.png)并初始化一组相应的模型权重![image](https://user-images.githubusercontent.com/48575896/226876784-3b3c31e8-e656-4bd1-b889-be2758d72817.png)

每个权值子模块是一个类似于全连接层的NeRF架构。

与Wild[21]中的NeRF类似，我们为每个用于计算辐射的输入图像 a 关联一个额外的外观嵌入向量l(a)。

这允许Mega-NeRF在解释图像之间的照明差异时增加了额外的灵活性，我们发现这在我们覆盖的场景的规模上是非常重要的。

在查询时，Mega-NeRF对于给定位置x、方向d和外观嵌入l(a)， NeRF使用最接近查询点的模型权重fn生成不透明度σ和颜色c = (r,g,b):

![image](https://user-images.githubusercontent.com/48575896/226881418-b7d01c53-248a-440e-a850-6c44c0c9ab36.png)

#### Centroid selection.
尽管我们探索了几种方法，包括k-means聚类和[44]中基于不确定性的分区，但我们最终发现，将场景分解为自上而下的2D网格在实践中效果很好。

这种方法实现简单，需要最少的预处理，并且能够在推理时有效地将点查询分配到质心。

由于在我们的场景中相机姿势之间的高度差异相对于纬度和经度的差异很小，我们将质心的高度固定为相同的值。

#### Foreground and background decomposition.
类似于nerf++[48]，我们进一步将场景细分为包含所有相机姿势的前景体和覆盖互补区域的背景。

这前景和背景两个卷都用单独的mega - nerf建模。

我们使用与nerf++相同的4D外部体积参数化和光线投射公式，但通过 使用更紧密地包围相机姿势和相关前景细节的 椭球 来  改进其单位球体划分。

我们还利用相机高度测量的优势，通过在地面附近终止射线来进一步细化场景的采样界限。

因此，mega - nerf可以更有效地避免不必要的地下区域和样本查询。

图3说明了两种方法之间的差异。

![image](https://user-images.githubusercontent.com/48575896/226882695-14a25a0b-fd11-493c-a176-b8537d2cc239.png)

### Training
#### Spatial Data Parallelism.
由于每个Mega-NeRF子模块都是一个独立的MLP，我们可以在没有模块间通信的情况下并行训练每个子模块。

至关重要的是，由于每张图像只捕捉了场景的一小部分(表1)，我们将每个子模块的训练集的大小限制为仅那些潜在相关的像素。

具体来说，我们沿着与每个训练图像的每个像素对应的摄像机射线采样点，并仅将该像素添加到它相交的空间单元中(图1)。
![image](https://user-images.githubusercontent.com/48575896/226888319-ad97aa49-71f6-4b6b-834e-e28a0a8344fe.png)

在我们的实验中，这种可见性划分将每个子模块的训练集的大小与初始聚合训练集相比减少了10倍。

对于更大规模的场景，这种数据减少应该更加极端;当训练北匹兹堡的NeRF时，不需要添加南匹兹堡的像素。我们在cells之间包括一个小的重叠因子(在我们的实验中为15%)，以进一步减少边界附近的视觉伪影。
