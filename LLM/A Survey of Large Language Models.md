# 大型语言模型综述全新出炉：从T5到GPT-4最全盘点，国内20余位研究者联合撰写
大型语言模型综述全新出炉：从T5到GPT-4最全盘点，国内20余位研究者联合撰写

自 20 世纪 50 年代图灵测试提出以来，人们始终在探索机器处理语言智能的能力。

语言本质上是一个错综复杂的人类表达系统，受到语法规则的约束。

因此，开发能够理解和精通语言的强大 AI 算法面临着巨大挑战。

过去二十年，语言建模方法被广泛用于语言理解和生成，包括统计语言模型和神经语言模型。

近些年，研究人员通过在大规模语料库上预训练 Transformer 模型产生了预训练语言模型（PLMs），并在解决各类 NLP 任务上展现出了强大的能力。

并且研究人员发现模型缩放可以带来性能提升，因此他们通过将模型规模增大进一步研究缩放的效果。

有趣的是，当参数规模超过一定水平时，这个更大的语言模型实现了显著的性能提升，并出现了小模型中不存在的能力，比如上下文学习。

为了区别于 PLM，这类模型被称为大型语言模型（LLMs）。

从 2019 年的谷歌 T5 到 OpenAI GPT 系列，参数量爆炸的大模型不断涌现。

