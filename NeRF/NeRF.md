# NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

## 摘要
我们提出了一种方法，通过使用稀疏的输入视图集优化underlying continuous volumetric scene function函数，实现了最先进的结果，用于合成复杂场景的新视图。

我们的算法使用全连接(非卷积)深度网络表示一个场景，其输入是一个连续的5D坐标(空间位置(x, y, z)和观看方向(θ, φ))，其输出是该空间位置的volume density和view-dependent emitted radiance。

我们通过沿着相机光线querying5D坐标来合成视图，并使用经典的体渲染技术将输出的颜色和密度投影到图像中。

因为体素渲染是自然可微的，所以优化我们的表示所需要的唯一输入是一组具有已知相机位姿的图像。

我们描述了如何有效地优化神经辐射场，以渲染具有复杂几何和外观的场景的逼真的新视图，并演示了优于先前在神经渲染和视图合成方面的工作的结果。

视图合成结果最好作为视频观看，因此我们敦促读者查看我们的补充视频，以便进行令人信服的比较。

## 介绍
  在这项工作中，我们以一种新的方式解决了长期存在的视图合成问题，直接优化连续5D场景表示的参数，以最大限度地减少渲染一组捕获图像的误差。

  我们将静态场景表示为一个连续的5D函数，它输出空间中每个点(x, y, z)在每个方向(θ， φ)上发射的辐射度(radiance emitted)，以及每个点上的密度，就像一个微分不透明度，通过光线经过点(x, y, z)来控制射线累积的辐射度。

我们的方法优化了一个深度全连接神经网络，没有任何卷积层(通常称为多层感知器或MLP)，通过从单个5D坐标(x, y, z， θ， φ)回归到单个体积密度和依赖于视图的RGB颜色来表示这个函数。

为了从特定的视点渲染神经辐射场(NeRF):
1)让摄像机光线穿过场景，生成一组采样的3D点
2)利用上诉3D点及其对应点二维观看方向作为神经网络的输入，以产生一组颜色和密度的输出，
3)使用经典的体素渲染技术将这些颜色和密度累积到2D图像中。

因为这个过程是自然可微的，我们可以使用梯度下降来优化这个模型，通过最小化每个观察到的图像和我们的表示所呈现的相应视图之间的误差。

通过将高体素密度和精确的颜色分配到包含真实底层场景内容的位置，最小化跨多个视图的这种误差，鼓励网络预测场景的一致模型。

  我们发现，优化复杂场景的神经辐射场representation的经典实现不会收敛到足够高的分辨率表示，并且在每个相机光线所需的样本数量方面效率低下。

我们通过使用位置编码转换输入5D坐标来解决这些问题，该编码使MLP能够表示更高频率的函数，并且我们提出了一种分层采样过程，以减少对这种高频场景表示进行充分采样所需的查询数量。

  我们的方法继承了体素表示的优点:两者都可以表示复杂的现实世界的几何和外观，并且非常适合使用投影图像进行基于梯度的优化。

至关重要的是，我们的方法克服了离散体素网格在高分辨率建模复杂场景时令人望而却步的存储成本。

综上所述，我们的技术贡献有:
  一种将具有复杂几何和材料的连续场景表示为5D神经辐射场的方法，参数化为基本MLP网络。
  一个基于经典体绘制技术的可微分渲染过程，我们用它来优化标准RGB图像的这些表示。这包括一个分层抽样策略，将MLP的容量分配给具有可见场景内容的空间。
  将每个输入5D坐标映射到高维空间的位置编码，使我们能够成功优化神经辐射场来表示高频场景内容。
  
  我们证明，我们得到的神经辐射场方法在定量和定性上优于最先进的视图合成方法，包括将神经3D表示适合场景的工作，以及训练深度卷积网络来预测采样的体积表示的工作。

据我们所知，本文提出了第一个连续神经场景表示，能够从自然环境中捕获的RGB图像中渲染真实物体和场景的高分辨率逼真新视图。

![image](https://user-images.githubusercontent.com/48575896/226278532-bc59807a-232f-49ab-bc04-746d4cf94294.png)

![image](https://user-images.githubusercontent.com/48575896/226281916-bd1ed524-08e5-4d9e-b248-475d5ea34849.png)

