# Segment Anything
# Abstract
我们介绍了Segment Anything (SA)项目:一个用于图像分割的新任务、模型和数据集。

在数据收集循环中使用我们的高效模型，我们构建了迄今为止(到目前为止)最大的分割数据集，在11M张授权和隐私尊重的图像上拥有超过10亿个掩码。

该模型被设计和训练为可提示的，因此它可以zero-shot转移到新的图像分布和任务。

我们评估了它在许多任务上的能力，发现它的zero-shot表现令人印象深刻, 经常与之前的完全监督结果竞争，甚至更好。

我们在上发布了1B掩模和11M图像的片段任意模型(SAM)和对应数据集(SA-1B)，以促进对计算机视觉基础模型的研究。https://segment-anything.com

# Introduction
在网络规模的数据集上预训练的大型语言模型正在以强大的零次和少次泛化彻底改变NLP.

这些“基础模型”可以泛化到训练过程中看不到的任务和数据分布。

这种功能通常通过提示工程实现，在提示工程中，使用手工制作的文本提示语言模型为手头的任务生成有效的文本响应。

当使用来自网络的丰富文本语料库进行缩放和训练时，这些模型的零镜头和少镜头性能与微调模型(在某些情况下甚至匹配)相比惊人地好。

经验趋势表明，这种行为随着模型规模、数据集大小和总训练计算而改善.

基础模型也在计算机视觉中进行了探索，尽管程度较轻。

也许最突出的插图是对齐来自网络的配对文本和图像。

例如，CLIP[82]和ALIGN[55]使用对比学习来训练对齐两种模式的文本和图像编码器。

经过训练后，经过设计的文本提示可以实现对新颖视觉概念和数据分布的零概率泛化。

这样的编码器还可以有效地与其他模块组合以实现下游任务，例如图像生成(例如DALL·E[83])。

虽然在视觉和语言编码器方面已经取得了很大的进展，但计算机视觉包括了超出这一范围的广泛问题，并且对于其中许多问题，不存在丰富的训练数据。

在这项工作中，我们的目标是建立一个图像分割的基础模型。

也就是说，我们试图开发一个prompt-able模型，并使用一种能够实现强大泛化的任务在广泛的数据集上对其进行预训练。

有了这个模型，我们的目标是用快速工程解决一系列新的数据分布上的下游分割问题。

该计划的成功取决于三个组成部分:任务、模型和数据。

为了开发它们，我们解决了以下关于图像分割的问题:

1. 什么任务可以实现zero-shot泛化?

2. 相应的模型架构是什么?

3. 哪些数据可以支持这个任务和模型?

这些问题错综复杂，需要综合解决。

我们首先定义一个可提示的分割任务，该任务足够普遍，可以提供一个强大的预训练目标，并支持广泛的下游应用。

这项任务需要一个模型，支持灵活的提示，并可以输出分割掩码实时提示时，允许交互式使用。

为了训练我们的模型，我们需要一个多样化的、大规模的数据源。

不幸的是，目前还没有用于分割的网络级数据源;为了解决这个问题，我们构建了一个“数据引擎”，即在使用我们的高效模型来辅助数据收集和使用新收集的数据来改进模型之间进行迭代。

接下来，我们将介绍每个相互连接的组件，然后是我们创建的数据集和证明我们方法有效性的实验。

## Task
在NLP和最近的计算机视觉中，基础模型是一个有前途的发展，它可以通过使用“提示”技术对新数据集和任务执行零次和少次学习。

受此工作的启发，我们提出了可提示的分割任务，其目标是给定任何分割提示，返回有效的分割掩码(见图1a)。

![image](https://user-images.githubusercontent.com/48575896/230816775-68936e78-d89c-4db4-adac-a3625e32dcee.png)

一个Prompt简单地指定在图像中分割什么，例如，提示符可以包括空间或文本信息标识一个对象。

有效输出掩码的要求意味着，即使提示符是模糊的，并且可能指向多个对象(例如，衬衫上的一个点可能表示衬衫或穿衬衫的人)，输出也应该是这些对象中至少一个的合理掩码。

我们使用提示分割任务作为预训练目标，并通过提示工程解决一般下游分割任务。

## Model 
可提示的分割任务和实际使用的目标对模型架构施加了约束。

特别地，该模型必须支持灵活的提示，需要实时平摊计算掩码以允许动态使用，并且必须具有歧义意识。

令人惊讶的是，我们发现一个简单的设计满足所有三个约束:一个强大的图像编码器计算图像嵌入，一个提示编码器嵌入提示，然后这两个信息源结合在一个轻量级的掩码解码器中，预测分割掩码。

我们把这个模型称为分段任意模型(Segment Anything model)，或SAM(见图1b)。

![image](https://user-images.githubusercontent.com/48575896/230816809-3b0db46d-1b79-4457-a532-493617c919ea.png)

通过将SAM分离为图像编码器和快速提示编码器/掩码解码器，可以使用不同的提示重用相同的图像嵌入(及其成本摊销)。

给定图像嵌入，提示编码器和掩码解码器在网络浏览器中从提示符中预测掩码，时间为~ 50ms。

我们主要关注点、框和掩码提示，并使用自由形式的文本提示来呈现初始结果。

为了使SAM能够感知歧义，我们将其设计为为单个提示预测多个掩码，允许SAM自然地处理歧义，例如衬衫vs.人的示例。

## Data engine
为了实现对新数据分布的强泛化，我们发现有必要在一个大而多样的掩码集上训练SAM，而不是现有的任何分割数据集。

虽然基础模型的典型方法是在线获取数据[82]，但掩码自然并不丰富，因此我们需要一种替代策略。

我们的解决方案是构建一个“数据引擎”，即我们与模型在循环数据集和符号共同开发我们的模型(见图1c)。

![image](https://user-images.githubusercontent.com/48575896/230816845-c13e548f-26f7-4795-ba47-a521e981b2b0.png)

我们的数据引擎有三个阶段:辅助手动、半自动和全自动。

在第一阶段，SAM帮助注释者注释掩码，类似于经典的交互式分割设置。

在第二阶段，SAM可以通过提示可能的对象位置来自动为对象子集生成掩码，而注释器则专注于注释剩余的对象，这有助于增加掩码多样性。

在最后阶段，我们用前景点的规则网格提示SAM，平均每张图像产生约100个高质量蒙版。

## Dataset 
我们最后的数据集SA-1B包含了超过1B掩码来自11M个许可和隐私保护图像(见图2)。

![image](https://user-images.githubusercontent.com/48575896/230817884-8b8483c6-8373-4ab0-a391-e28663772b5d.png)

除了用于训练SAM的健壮性和通用性之外，我们希望SA-1B能够成为一种有价值的资源，用于建立新的基础模型。

## Responsible AI
在使用SA-1B和SAM时，我们研究并报告潜在的公平性问题和偏差。

SA-1B中的图像跨越了地理和经济上不同的国家，我们发现SAM在不同人群中表现相似。

总之，我们希望这将使我们的工作在现实用例中更加公平。

我们在附录中提供了模型和数据集卡。

## Experiments
我们广泛地评估SAM。

首先，我们采用23个分割数据集的多样化新套件，我们发现SAM从单个前景点生成高质量的掩模，通常仅略低于人工注释的地面真相。

其次，我们在使用提示工程的零镜头传输协议下的各种下游任务上发现了始终强大的定量和定性结果，包括边缘检测、对象提议生成、实例分割和文本到掩码预测的初步探索。

这些结果表明，SAM可以与即时工程一起使用，以解决涉及SAM训练数据之外的对象和图像分布的各种任务。

尽管如此，正如我们在§8中讨论的，改进的空间仍然存在。

# Segment Anything Task
我们从NLP中获得灵感，其中下一个令牌预测任务用于基础模型预训练，并通过提示工程师[10]解决各种下游任务。

为了建立一个分割的基础模型，我们的目标是定义一个具有类似功能的任务。

## Task
我们首先将提示的思想从NLP转换为分割，在分割中提示可以是一组前景/背景点，一个粗略的框或蒙版，自由形式的文本，或者，一般来说，任何表明在图像中分割什么的信息。

那么，提示分割任务是在给定任何提示时返回一个有效的分割掩码。

“有效”掩码的要求仅仅意味着即使提示符是模糊的，并且可以引用多个对象(例如，回想一下衬衫vs.人的例子，见图3)，输出应该是这些对象中至少一个的合理掩码。

![image](https://user-images.githubusercontent.com/48575896/230819431-74dab0bb-1b8c-4a39-8bfa-9b34155bf6a0.png)

这一需求类似于期望语言模型对模棱两可的提示符输出一致的响应。

我们选择这个任务是因为它导致了一个自然的预训练算法和一个通过提示将零镜头转移到下游分割任务的通用方法.

## Pre-training
提示分割任务提出了一种自然的预训练算法，该算法为每个训练样本模拟一系列提示(例如，点、框、掩码)，并将模型的掩码预测与地面真相进行比较。

我们从交互式分割[109,70]中采用了这种方法，尽管与交互式分割的目标是在足够的用户输入后最终预测有效掩码不同，我们的目标是始终预测任何提示的有效掩码，即使提示是模糊的。

这确保了预训练的模型在涉及歧义的用例中是有效的，包括我们的数据引擎§4所要求的自动注释。

我们注意到，在这项任务中表现良好具有挑战性，需要专门的建模和训练损失选择，我们将在§3中讨论。

## Zero-shot transfer
直观地说，我们的预训练任务赋予模型在推理时对任何提示作出适当响应的能力，因此可以通过设计适当的提示来解决下游任务。

例如，如果有一个猫的边界盒检测器，猫实例分割可以通过提供检测器的盒子输出作为我们模式的提示来解决.

一般来说，大量的实际分割任务都可以作为提示。

除了自动数据集标记之外，我们还在§7的实验中探索了五个不同的示例任务。

## Related tasks.
分割是一个广泛的领域:有动态分割[57,109]、边缘检测[3]、素像素化[85]、目标提议生成[2]、前景分割[94]、语义分割[90]、原地分割[66]、全景分割[59]等。

我们的提示分割任务的目标是产生一个广泛的能力模型，可以适应许多(虽然不是全部)现有的和新的分割任务通过提示工程。

这种能力是任务概括的一种形式[26]。

注意，这与以前的多任务分割系统不同。

在多任务系统中，单个模型执行一组固定的任务，例如联合语义、实例和全景分割[114,19,54]，但训练和测试任务是相同的。

在我们的工作中，一个重要的区别是，为提示分割训练的模型可以在推理时作为一个更大的系统中的组件执行一个新的、不同的任务，例如，为了执行实例分割，提示分割模型与现有的对象检测器相结合。

## Discussion
提示和组合是强大的工具，可以以可扩展的方式使用单个模型，有可能完成模型设计时未知的任务。

这种方法类似于其他基础模型的使用方式，例如CLIP[82]是DALL·E[83]图像生成系统的文本图像对齐组件。

我们预计，由提示工程等技术驱动的可组合系统设计，将比专门为一组固定任务训练的系统能够实现更广泛的应用。

通过组成的镜头来比较提示分割和交互式分割也是有趣的:虽然交互式分割模型是在设计时考虑到人类用户，但为提示分割训练的模型也可以组成一个更大的算法系统，正如我们将演示的那样。

# Segment Anything Model
接下来我们描述分段任意模型(SAM)用于提示分段。

SAM有三个组成部分，如图4所示:一个图像编码器，一个灵活的提示编码器和一个快速的掩码解码器。

我们建立在Transformer视觉模型[14,33,20,62]的基础上，为(平摊)实时性能。

我们在这里对这些组件进行高层次的描述，详细内容见§a。

![image](https://user-images.githubusercontent.com/48575896/230822655-2afda656-c8f8-46cc-8218-8ec6ff387654.png)

## Image encoder
由于可扩展性和强大的预训练方法，我们使用MAE[47]预训练视觉变压器(ViT)[33]最小限度地适应于处理高分辨率输入[62]。

图像编码器对每张图像运行一次，可以在提示模型之前应用.

## Prompt encoder
我们考虑两组提示:稀疏提示(点、框、文本)和密集(蒙版)。

我们通过位置编码[95]表示点和框，并对每种提示类型和自由形式的文本使用CLIP现成的文本编码器[82]进行学习嵌入。

密集提示(即掩码)使用卷积嵌入，并与图像嵌入元素相加。

## Mask decoder
掩码解码器有效地将图像嵌入、提示嵌入和输出令牌映射到掩码。

该设计受到[14,20]的启发，采用了对变压器解码器块[103]的修改，然后是动态掩码预测头。

我们改进的解码器块在两个方向上使用提示自注意和交叉注意(提示到图像嵌入和反之亦然)来更新所有嵌入。

在运行两个块之后，我们对图像嵌入进行上采样，MLP将输出令牌映射到动态线性分类器，然后在每个图像位置计算掩码前景概率。

## Resolving ambiguity
对于一个输出，如果给出模棱两可的提示，模型将平均显示多个有效掩码。

为了解决这个问题，我们修改模型以预测单个提示符的多个输出掩码(见图3)。

我们发现3个掩码输出足以解决大多数常见情况(嵌套的掩码通常最多有三个深度:整体、部分和子部分)。

在训练期间，我们只在面具上反向传播最小的损失[15,45,64]。

为了对面具进行排名，模型预测每个面具的置信度分数(即估计的IoU)。

## Efficiency
整个模型设计很大程度上是由效率驱动的。

给定一个预先计算的图像嵌入，提示编码器和掩码解码器在web浏览器上运行CPU，单位:~ 50ms。

这种运行时性能使我们的模型无接缝、实时交互式提示。

## Losses and training.
我们使用[14]中使用的 focal loss[65]和 dice loss[73]的线性组合来监督掩模预测。

我们使用混合的几何提示来训练提示分割任务(关于文本提示，请参阅§7.5)。

按照[92,37]，我们通过在每个掩码中随机抽取11轮提示来模拟交互式设置，允许SAM无缝集成到我们的数据引擎中。

# Segment Anything Data Engine
由于网络上的分割掩码并不多，我们建立了一个数据引擎来收集我们的分割掩码1.1B掩码数据集，SA-1B。

数据引擎分为三个阶段:

(1)模型辅助人工标注阶段;

(2)混合了自动预测掩码和模型辅助注释的半自动阶段

(3)一个完全自动的阶段，在这个阶段，我们的模型无需注释器输入即可生成掩码。

接下来我们将详细介绍每一个。

## Assisted-manual stage
在第一阶段，类似于分类交互分割，一队专业注释人员使用由SAM支持的基于浏览器的交互式分割工具，通过单击前景/背景对象点来标记掩码。

蒙版可以使用像素精确的“笔刷”和“橡皮擦”工具进行细化。

我们的模型辅助注释直接在浏览器内实时运行(使用预先计算的图像嵌入)，从而实现真正的交互体验。

我们没有对标记对象施加语义约束，注释者可以自由地标记“stuff”和“things”[1]。

我们建议注释者标记他们可以命名或描述的对象，但没有收集这些名称或描述。

标注者被要求按突出的顺序标注对象，并被鼓励在一个蒙版花费30秒以上的时间来标注下一个图像。

在这一阶段的开始，SAM使用公共分割数据集进行训练。

在足够的数据标注后，仅使用新标注的掩码重新训练SAM。

随着越来越多的掩模被收集，图像编码器从ViT-B 缩放到ViT-H和其他架构细节的演变;我们总共重新训练了我们的模型6次。

随着模型的改进，每个掩码的平均注释时间从34秒减少到14秒。

我们注意到，14秒比COCO的掩码标注快6.5倍[66]，仅比extreme points的边界框标注慢2倍[76,71]。

随着SAM的改进，每张图像的平均掩码数量从20个增加到44个。

总的来说，我们在这个阶段从12万张图片中收集了430万个面具。

## Semi-automatic stage
在这个阶段，我们的目标是增加面具的多样性，以提高我们的模型分割任何东西的能力。

为了将注释器集中在不太突出的对象上，我们首先自动检测自信掩码。

然后，我们向注释者展示了预先填充了这些蒙版的图像，并要求他们注释任何其他未注释的对象。

为了检测可信掩码，我们使用通用的“对象”类别在所有第一阶段掩码上训练了一个边界盒检测器[84]

在此阶段，我们在180k图像中收集了额外的590万个面具(总共1020万个面具)。

与第一阶段一样，我们定期对新收集的数据重新训练我们的模型(5次)。

每个掩码的平均注释时间回到34秒(不包括自动掩码)，因为这些对象的标签更具挑战性。

每张图片的平均蒙版数量从44个增加到72个(包括自动蒙版)。

## Fully automatic stage
在最后阶段，注释是完全自动的。

这是可行的，因为我们的模型有两个主要的改进。

首先，在这一阶段的开始，我们收集了足够多的面具来极大地改进模型，包括前一阶段的各种面具。

第二，在这个阶段，我们已经开发了模糊感知模型，它允许我们预测有效的掩码，即使在不明确的情况下。

具体地说，我们用32×32规则网格的点和每个点预测一组掩码，可能对应于有效的对象。

对于歧义感知模型，如果一个点位于一个部件或子部件上，我们的模型将返回子部件、部分和整个对象。

利用模型中的IoU预测模块来选择有信心的掩码;此外，我们只识别和选择了稳定的掩码(如果阈值在0.5−δ和0.5 + δ处的概率图产生相似的掩码，我们认为掩码是稳定的)。

最后，在选择自信和稳定的掩码后，应用非最大抑制(non- maximum suppression, NMS)对重复数据进行过滤。

为了进一步提高较小蒙版的质量，我们还处理了多个重叠的放大图像作物。

关于这一阶段的更多细节，请参见§B。

我们对数据集中的所有11M张图像应用了全自动蒙版生成，总共生成了1.1亿个高质量的蒙版。

接下来，我们描述并分析结果数据集SA-1B。

# Segment Anything Dataset
我们的数据集，SA-1B，由11M不同的，高分辨率，授权和隐私保护图像和1.1B用我们的数据引擎收集的高质量分割掩码。

我们将SA-1B与现有数据集进行比较，并分析掩模质量和性能。

我们正在释放SA-1B帮助未来计算机视觉基础模型的发展。

我们注意到SA-1B将在有利的许可协议下发布，用于某些研究用途，并保护研究人员。

## Images.
我们从一家直接与摄影师合作的提供商那里获得了一组新的1100万张图片的授权。

这些图像是高分辨率的(平均为3300×4950像素)，由此产生的数据大小可能会带来可访问性和存储的挑战。

因此，我们将发布下采样图像，并将其最短边设置为1500像素。

即使在降采样之后，我们的图像的分辨率也明显高于许多现有的视觉数据集(例如，COCO[66]图像的像素为480×640)。

请注意，目前大多数模型都在低分辨率输入上运行。

在公布的图片中，人脸和车牌被模糊处理。

## Masks.
我们的数据引擎生产了11亿个口罩，其中99.1%是全自动生成的。

因此，自动掩模的质量是至关重要的。

我们将它们直接与专业注释进行比较，并查看各种掩码属性与突出的分割数据集进行比较。

正如下面的分析和§7的实验所证明的那样，我们的主要结论是，我们的自动掩模对于训练模型来说是高质量和有效的。

基于这些发现，SA-1B只包含自动生成的掩码。

## Mask quality
为了评估蒙版质量，我们随机抽取了500张图片(约50k蒙版)，并要求我们的专业注释人员提高这些图像中所有蒙版的质量。

注释器使用我们的模型和像素精确“画笔”和“橡皮擦”编辑工具。

这个过程产生了一对自动预测和专业校正的面具。

我们计算了每对之间的IoU，发现94%的IoU超过90%, 97%的IoU超过75%。

相比之下，先前的工作估计注释器之间的一致性为85-91%IoU[44,60]。

我们在§7中的实验通过人类评级证实，相对于各种数据集，掩码质量都很高，并且在自动掩码上训练我们的模型几乎与使用数据引擎产生的所有掩码一样好。

## Mask properties.

![image](https://user-images.githubusercontent.com/48575896/230838673-6b5ae887-8df6-4664-84c7-7c1cdf798e48.png)

在图5中，我们绘制了SA-1B中对象中心的空间分布，与最大的现有分割数据集进行了比较。

所有数据集中都存在常见的摄影师偏见。

我们观察到SA-1B比LVIS v1[44]和有更大的图像角覆盖ADE20K[117]是两个分布最相似的数据集，而COCO[66]和Open Images V5[60]具有更突出的中心偏倚。

![image](https://user-images.githubusercontent.com/48575896/230839893-18933a58-252c-4622-adcb-49422c07673e.png)

在图6(图例)中，我们通过大小来比较这些数据集。

SA-1B比第二大的Open images多出11倍的图像和400倍的掩模。

在平均，它有36×more掩码每个图像比Open Images。

在这方面最接近的数据集ADE20K，每张图像的掩码仍然少3.5倍。

图6(左)绘制了每张图像的掩模分布。

接下来，我们看看图6(中间)中图像相对掩码大小(掩码面积除以图像面积的平方根)。

正如预期的那样，由于我们的数据集每张图像有更多的遮罩，它也倾向于包括更大比例的中小型相对大小的遮罩。

最后，为了分析形状复杂性，我们看图6(右)中的掩模凹度(1减去掩模面积除以掩模凸包面积)。

由于形状复杂性与掩码大小相关，我们通过首先从分类掩码大小中执行分层采样来控制数据集的掩码大小分布。

我们观察到掩模的凹面分布与其他数据集的凹面分布大致相似。

# Zero-Shot Transfer Experiments
在本节中，我们将介绍SAM(分段任意模型)的零镜头转移实验。

我们考虑五个任务，其中四个与用于训练SAM的提示分割任务有显著不同。

这些实验在训练期间没有看到的数据集和任务上评估SAM(我们使用“零镜头转移”遵循其在CLIP中的使用[82])。

他的数据集可能包括新颖的图像分布，例如水下或以自我为中心的图像(例如。图8)，据我们所知，在SA-1B中没有出现。

我们的实验从测试提示分割的核心目标开始:从任何提示生成有效的掩码。

我们强调单前景点提示的挑战性场景，因为它比其他更具体的提示更可能是模糊的。

接下来，我们提出了一系列穿越低、中、高水平图像理解的实验，并大致平行于该领域的历史发展。

具体来说，我们提示SAM:

(1)进行边缘检测

(2)分割一切，即对象提议生成，

(3)对检测到的对象进行分段，即实例分割

(4)作为概念证明，从自由形式的文本中分割对象。

这四个任务明显不同于提示分割任务SAM是通过快速工程进行培训和实施的。

我们的实验以消融研究结束.

## Implementation
除非另有说明:

(1) SAM使用MAE[47]预训练的ViT-H[33]图像编码器

(2) SAM在SA-1B上进行训练，注意到该数据集仅包括来自数据引擎最后阶段的自动生成的掩码。

所有其他模型和训练细节，如超参数，请参考§A。

# 7.1. Zero-Shot Single Point Valid Mask Evaluation
## Task
我们评估从单个前景点分割一个对象。

这个任务是不恰当的，因为一个点可以指向多个对象。

大多数数据集中的地面真相掩码没有列举所有可能的掩码，这可能使自动度量不可靠。

因此，我们用一项人类研究补充了标准mIoU度量(即预测的和真实掩模之间的所有IoU的平均值)，其中注释者将掩模质量从1(无意义)评分到10(像素完美)。

默认情况下，我们从地面真相掩码的“中心”(掩码内部距离变换的最大值)采样点，遵循交互式分割中的标准评估协议[92]。

由于SAM能够预测多个掩码，我们默认只评估模型中最可信的掩码.

基线都是单掩码方法。

我们主要与RITM[92]进行比较，RITM是一种强交互分段器，与其他强基线相比，它在我们的基准上表现最好[67,18]。

## Datasets
我们使用一个新编译的23个数据集的套件，具有不同的图像分布。

图8列出了数据集，并显示了每个数据集的示例(详情见附录表7)。

![image](https://user-images.githubusercontent.com/48575896/230850490-fe57d238-0512-4dca-a798-1102b65a77f0.png)

我们使用所有23个数据集进行mIoU评估。

对于人体研究，我们使用图9b中列出的子集(由于这类研究需要资源)。

![image](https://user-images.githubusercontent.com/48575896/230850844-f600fa42-960c-42c0-91d0-b06aaa685dd4.png)

这个子集包括两个数据集，根据自动指标，SAM优于和低于RITM。

## Results
首先，我们使用mIoU对23个数据集的完整套件进行自动评估。

我们将图9a中的每个数据集结果与RITM进行比较。

![image](https://user-images.githubusercontent.com/48575896/230851525-1170361a-2f6e-4a33-b25b-15d2d484349e.png)

SAM在23个数据集中的16个上产生了更高的结果，高达47个IoU。

我们还提出了一个“oracle”结果，其中SAM的3个掩码通过与地面真相进行比较来选择最相关的掩码，而不是选择最可信的掩码。

这揭示了歧义对自动评估的影响。

特别是，在oracle执行歧义解析的情况下，SAM在所有数据集上都优于RITM。

人体研究结果如图9b所示。

误差条是平均掩码评级的95%置信区间(所有差异均显著;详情见§E)。

我们观察到注释者一致地评价SAM的掩模明显高于最强的基线RITM。

使用单一输出掩码的SAM的消隐、“不识别歧义”版本的评分始终较低，但仍然高于RITM。

SAM的平均评级在7到9之间，这与定性评级准则相对应:“高分(7-9):物体是可识别的，错误小而罕见。

这些结果表明SAM已经学会从单点分割有效的掩码。

注意，对于像DRAM和在IBD中，SAM在自动指标上较差，但在人类研究中得到了一贯较高的评分。

图9c显示了附加的基线、SimpleClick[67]和FocalClick[18]，其单点性能低于RITM和SAM。

![image](https://user-images.githubusercontent.com/48575896/230856350-065a6f06-0ee9-4de9-bb98-41b742ccefd7.png)

当折痕中的点数从1增加到9时，我们观察到冰毒之间的差距减小。

当任务变得更容易时，这是预期的;此外，SAM也没有针对非常高的IoU制度进行优化。

最后，在图9d中，我们将默认的中心点采样替换为随机点采样。

我们观察到SAM和基线之间的差距越来越大，SAM在两种采样方法下都能获得类似的结果。

![image](https://user-images.githubusercontent.com/48575896/230856664-e761d907-47fa-4ca4-b289-a323a533558c.png)

# 7.2. Zero-Shot Edge Detection
## Approach
我们使用BSDS500[72,3]评估了SAM在经典的低层次边缘检测任务上的性能。

我们使用我们的自动掩模生成管道的模拟化版本。

具体来说，我们用前景点的16×16规则网格提示SAM，结果是768个预测掩码(每点3个)。

NMS移除冗余的掩码。

然后，使用未阈值掩码概率映射的Sobel滤波和标准的轻量级后处理计算边缘映射，包括边缘NMS(参见§D)。2)。

## Results
我们在图10中可视化代表性的边缘映射(详见图15)。

定性地说，我们观察到，即使SAM没有经过边缘检测训练，它也能产生合理的边缘图。

与地面真相相比，SAM预测了更多的边，包括BSDS500中没有标记的敏感边。

这种偏见在数量上反映在表3:50%精度(R50)的召回率很高，但以精度为代价。

SAM自然落后于学习BSDS500偏差的最先进的兴奋剂，即要压制的边缘。

尽管如此，SAM与HED[108](也在BSDS500上训练)等开创性的深度学习方法相比表现良好，并且明显优于先前的零次传输方法，尽管公认已经过时。

# 7.3. Zero-Shot Object Proposals
## Approach
接下来，我们在对象建议生成的中层任务上评估SAM[2,102]。

这项任务在目标检测研究中发挥了重要作用，作为开创性系统的中间步骤(例如，[102,41,84])。

为了生成对象建议，我们运行一个稍微修改过的自动掩码生成管道，并将掩码作为建议输出(参见§D)。3)。

我们计算标准平均召回(AR)指标LVIS v1[44]。

我们专注于LVIS，因为它的大量类别提供了一个具有挑战性的测试。

我们将其与作为ViTDet[62]检测器实现的强基线进行比较(with cascade Mask R-CNN [48, 11] ViT-H).。

我们注意到这个“基线”对应于“探测器伪装成提议生成器”(DMP)方法[16]，这是一个真正苛刻的比较。

## Results
![image](https://user-images.githubusercontent.com/48575896/230860304-a4460717-643b-438d-b1e4-cced472f3bc7.png)

在表4中，我们毫不意外地看到，使用ViTDet-H的检测作为对象建议(即DMP方法[16]认为游戏AR)总体上表现最好。

然而，SAM在几个方面做得非常好。

值得注意的是，它在中型和大型对象以及罕见和常见对象上优于ViTDet-H。

事实上,SAM只在小对象和频繁对象上表现不如ViTDet-H，其中ViTDet-H可以很容易地学习特定于LVIS的注释偏差，因为它是在LVIS上训练的，不像SAM。

我们还与SAM(“单挑”)的消除歧义不意识版本进行了比较，它在所有AR指标上的表现都明显比SAM差。

# 7.4. Zero-Shot Instance Segmentation
## Approach
转到更高层次的视觉，我们使用SAM作为实例分段器的分段模块。

实现很简单:我们运行一个对象检测器之前使用的ViTDet)，并用输出框提示SAM。

这演示了在一个更大的系统中组合SAM。

## Results
我们比较了SAM和预测的面具表5中COCO和LVIS的ViTDet。

![image](https://user-images.githubusercontent.com/48575896/230863116-b9e670f2-b3c9-4777-b2ba-3288aa79376b.png)

看看掩码AP指标，我们观察到两个数据集上的差距，其中SAM相当接近，虽然肯定落后于ViTDet。

通过可视化输出，我们观察到SAM掩模通常在质量上优于ViTDet，具有更清晰的边界(见§D)。4和图16)。

为了调查这一观察结果，我们进行了一项额外的人体研究，要求注释者对ViTDet掩码和SAM掩码进行评分以前使用的1到10的质量量表。

在图11中我们观察到SAM在人体研究中一直优于ViTDet。

我们假设在COCO上，掩码AP差距较大，地面真相质量相对较低(由人类研究证实)，ViTDet学习了COCO掩码的特定偏差。

SAM作为一种零概率方法，无法利用这些(通常不受欢迎的)偏差。

LVIS数据集具有更高质量的地面真相，但仍然存在特定的特质(例如，掩模不包含孔，它们是构造上的简单多边形)和模态掩模与模态掩模的偏差。

同样，SAM没有接受过学习这些偏见的训练，而ViTDet可以利用它们。

# 7.5. Zero-Shot Text-to-Mask
## Approach
最后，我们考虑一个更高级的任务:从自由形式的文本中分割对象。

这个实验是SAM处理文本提示能力的概念证明。

虽然我们在之前的所有实验中都使用了完全相同的SAM，但对于这个SAM的训练过程进行了修改，使其能够识别文本，但在某种程度上不需要新的文本注释。

具体来说，对于每个人工采集的面积大于100^2的掩模，我们提取CLIP图像嵌入。

然后，在训练过程中，将提取的CLIP图像嵌入作为SAM的第一个动作进行提示。

这里的关键观察是，由于CLIP的图像嵌入被训练为与文本嵌入对齐，所以我们可以使用图像嵌入进行训练，但使用文本嵌入进行推理。

也就是说，在推理时，我们通过CLIP的文本编码器运行文本，然后将生成的文本嵌入作为提示给SAM(见§D)。5)。

## Results
我们在图12中显示了定性结果。

SAM可以根据简单的文本提示(如“车轮”)和短语(如“海狸齿格栅”)来分割对象。

当SAM不能从文本提示中选择正确的对象，一个额外的点通常会修复预测，类似于[31]。

# Ablations
我们使用单中心点提示协议在我们的23数据集套件上执行了几次消融。回想一下，单个点可能是模糊的，模糊可能无法在基本真理中表示，因为每个点只包含一个掩码。

由于SAM是在零射转移设置下操作的，因此在两者之间可能存在系统偏差SAM的顶级掩码与数据注释准则产生的掩码。

因此，我们额外报告了与地面真相(“神谕”)有关的最佳掩模。

![image](https://user-images.githubusercontent.com/48575896/230866689-873b2aae-bf14-445c-8439-3b1a589b18a1.png)

图13(左)显示了SAM在数据引擎阶段累积数据训练时的性能。

我们观察到每一个阶段mIoU都在增加。

当进行所有三个阶段的训练时，自动面具的数量远远超过手动和半自动面具。

为了解决这个问题，我们发现在训练过程中对手动和半自动口罩进行10倍的过采样会得到最好的结果。

这种设置使培训变得复杂。

因此，我们测试了仅使用自动生成的掩码的第四种设置。

使用这些数据，SAM的性能仅略低于使用所有数据(~ 0.5 mIoU)。

因此，默认情况下，我们只使用自动生成的掩码来简化训练设置。

在图13(中)中，我们看看数据量的影响。

完整的SA-1B包含11M图像，我们将其均匀地细分为1M和0.1M用于消融。

在0.1M成像时，我们观察到在所有设置下mIoU都有较大的下降。

然而，对于1M图像，大约占整个数据集的10%，我们观察到的结果与使用整个数据集相当。

这个数据制度仍然包括大约100M个掩码，对于许多用例来说可能是一个实用的设置。

最后，图13(右)显示了使用vitb、vitl和vith图像编码器的结果。vith比vitb有显著改善，但仅比vitl有轻微提高。进一步的图像编码器缩放目前看来没有成效。

# Discussion
