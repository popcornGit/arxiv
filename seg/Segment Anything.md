# Segment Anything
# Abstract
我们介绍了Segment Anything (SA)项目:一个用于图像分割的新任务、模型和数据集。

在数据收集循环中使用我们的高效模型，我们构建了迄今为止(到目前为止)最大的分割数据集，在11M张授权和隐私尊重的图像上拥有超过10亿个掩码。

该模型被设计和训练为可提示的，因此它可以zero-shot转移到新的图像分布和任务。

我们评估了它在许多任务上的能力，发现它的zero-shot表现令人印象深刻, 经常与之前的完全监督结果竞争，甚至更好。

我们在上发布了1B掩模和11M图像的片段任意模型(SAM)和对应数据集(SA-1B)，以促进对计算机视觉基础模型的研究。https://segment-anything.com

# Introduction
在网络规模的数据集上预训练的大型语言模型正在以强大的零次和少次泛化彻底改变NLP.

这些“基础模型”可以泛化到训练过程中看不到的任务和数据分布。

这种功能通常通过提示工程实现，在提示工程中，使用手工制作的文本提示语言模型为手头的任务生成有效的文本响应。

当使用来自网络的丰富文本语料库进行缩放和训练时，这些模型的零镜头和少镜头性能与微调模型(在某些情况下甚至匹配)相比惊人地好。

经验趋势表明，这种行为随着模型规模、数据集大小和总训练计算而改善.

基础模型也在计算机视觉中进行了探索，尽管程度较轻。

也许最突出的插图是对齐来自网络的配对文本和图像。

例如，CLIP[82]和ALIGN[55]使用对比学习来训练对齐两种模式的文本和图像编码器。

经过训练后，经过设计的文本提示可以实现对新颖视觉概念和数据分布的零概率泛化。

这样的编码器还可以有效地与其他模块组合以实现下游任务，例如图像生成(例如DALL·E[83])。

虽然在视觉和语言编码器方面已经取得了很大的进展，但计算机视觉包括了超出这一范围的广泛问题，并且对于其中许多问题，不存在丰富的训练数据。

在这项工作中，我们的目标是建立一个图像分割的基础模型。

也就是说，我们试图开发一个prompt-able模型，并使用一种能够实现强大泛化的任务在广泛的数据集上对其进行预训练。

有了这个模型，我们的目标是用快速工程解决一系列新的数据分布上的下游分割问题。

该计划的成功取决于三个组成部分:任务、模型和数据。

为了开发它们，我们解决了以下关于图像分割的问题:

1. 什么任务可以实现zero-shot泛化?

2. 相应的模型架构是什么?

3. 哪些数据可以支持这个任务和模型?
