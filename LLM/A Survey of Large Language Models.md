# 大型语言模型综述全新出炉：从T5到GPT-4最全盘点，国内20余位研究者联合撰写
大型语言模型综述全新出炉：从T5到GPT-4最全盘点，国内20余位研究者联合撰写

自 20 世纪 50 年代图灵测试提出以来，人们始终在探索机器处理语言智能的能力。

语言本质上是一个错综复杂的人类表达系统，受到语法规则的约束。

因此，开发能够理解和精通语言的强大 AI 算法面临着巨大挑战。

过去二十年，语言建模方法被广泛用于语言理解和生成，包括统计语言模型和神经语言模型。

近些年，研究人员通过在大规模语料库上预训练 Transformer 模型产生了预训练语言模型（PLMs），并在解决各类 NLP 任务上展现出了强大的能力。

并且研究人员发现模型缩放可以带来性能提升，因此他们通过将模型规模增大进一步研究缩放的效果。

有趣的是，当参数规模超过一定水平时，这个更大的语言模型实现了显著的性能提升，并出现了小模型中不存在的能力，比如上下文学习。

为了区别于 PLM，这类模型被称为大型语言模型（LLMs）。

从 2019 年的谷歌 T5 到 OpenAI GPT 系列，参数量爆炸的大模型不断涌现。

可以说，LLMs 的研究在学界和业界都得到了很大的推进，尤其去年 11 月底对话大模型 ChatGPT 的出现更是引起了社会各界的广泛关注。

LLMs 的技术进展对整个 AI 社区产生了重要影响，并将彻底改变人们开发和使用 AI 算法的方式。

考虑到 LLMs 的快速技术进步，中国人民大学的二十几位研究者通过背景知识、关键发现和主流技术等三方面回顾了 LLMs 的最新进展，尤其关注 LLMs 的预训练、自适应调优、使用和能力评估。

此外他们还总结和开发 LLMs 的可用资源，讨论了未来发展方向等问题。对于领域内研究人员和工程师而言，这份综述是一份极其有用的学习资源。

![image](https://user-images.githubusercontent.com/48575896/229475367-231b5cfb-f042-4982-8279-94aa61921eb3.png)

论文链接：https://arxiv.org/abs/2303.18223

在进入正文前，我们先来看 2019 年以来出现的各种大语言模型（百亿参数以上）时间轴，其中标黄的大模型已开源。

![image](https://user-images.githubusercontent.com/48575896/229475469-4b5efdee-b155-4ae0-82f1-77f4d8cb3964.png)

在第一节中，研究者详细介绍了 LLMs 的背景、能力和关键技术。

## LLMs 的背景
通常，大型语言模型（LLM）是指包含数千亿（或更多）参数的语言模型，这些参数是在大量文本数据上训练的，例如模型 GPT-3、PaLM、Galactica 和 LLaMA。

具体来说，LLM 建立在 Transformer 架构之上，其中多头注意力层堆叠在一个非常深的神经网络中。

现有的 LLM 主要采用与小语言模型类似的模型架构（即 Transformer）和预训练目标（即语言建模）。

作为主要区别，LLM 在很大程度上扩展了模型大小、预训练数据和总计算量（扩大倍数）。

他们可以更好地理解自然语言，并根据给定的上下文（例如 prompt）生成高质量的文本。

这种容量改进可以用标度律进行部分地描述，其中性能大致遵循模型大小的大幅增加而增加。

这种容量改进可以用标度律进行部分地描述，其中性能大致遵循模型大小的大幅增加而增加。

