# 4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions
## Abstract
在本文中，我们提出了一种新颖而有效的框架，称为4K-NeRF，以神经辐射场(NeRF)的方法为基础，在超高分辨率的挑战性场景上追求高保真视图合成。

基于nerf方法的渲染过程通常依赖于一种像素方式，在这种方式中，射线(或像素)在训练和推断阶段都是独立处理的，这限制了其描述微妙细节的表示能力，特别是当提升到极高分辨率时。

我们通过更好地探索射线相关性来解决这个问题，以增强受益于几何感知局部上下文的高频细节。

特别地，我们使用视域一致编码器在低分辨率空间中有效地建模地测量信息，并通过视域一致解码器恢复细节，条件是编码器估计的光线特征和深度。

联合训练和基于补丁的采样进一步促进了我们的方法，结合了超越像素损失的面向感知的正则化监督。

与现代NeRF方法的定量和定性比较表明，我们的方法可以显著提高保留高频细节的渲染质量，在4K超高分辨率场景中实现最先进的视觉质量。

## Introduction
超高分辨率作为记录和显示图像和视频的标准越来越受欢迎，甚至在现代移动设备中也得到了支持。

与使用相对较低的分辨率相比，超高分辨率格式捕捉的场景通常会呈现令人难以置信的内容细节(例如，1K高清格式)，其中像素的信息在极高分辨率的图像中通过一个小补丁放大。

开发处理这些高频细节的技术对图像处理和计算机视觉的广泛任务提出了挑战。

在本文中，我们重点研究了一种新颖的视图合成任务，并研究了在超高分辨率下实现富含细微细节的高保真视图合成的潜力。

新视图合成的目标是从多个视点捕获的稀疏图像集，生成自由视图的真实感合成。

最近，Neural Radi- ance Fields[26]提供了一种利用深度神经网络建模和渲染3D场景的新方法，与传统的视图插值方法相比，在提高视觉质量方面取得了显著的成功[37,37,45]。

其中，优化了深度多层感知器(MLP)的映射函数，将给定的观看方向的每个三维位置与其对应的亮度颜色和体积密度相关联，而实现视点依赖效果需要对每个像素的光线投射进行数百次查询。

几种后续的方法被提出来改进该方法，无论是从减少多尺度上的混叠伪影[1]方面，还是从使用离散化结构提高训练和推理效率[6,36,46]。

所有这些方法都遵循像素级机制，尽管架构不同，即在训练和推理阶段对不同的射线独立地处理。

它们通常是在1K分辨率训练视图上开发的.

当将该方法应用于超高分辨率场景时，由于捕捉精细细节的表征能力不足，会出现令人反感的模糊工件(如图3所示)。
![image](https://user-images.githubusercontent.com/48575896/227127104-13779fb3-c1fa-4d69-ad14-e304fe357ab6.png)

在本文中，我们介绍了一个新的框架，命名为4K-NeRF的体绘制方法，实现4K超高分辨率的高保真视图合成。

我们从卷积神经网络在传统超分辨率上的成功中获得灵感[11]，通过学习相邻像素之间的局部先验，将较低分辨率的观测结果解析为具有丰富细节的较高分辨率。

我们希望通过更好地探索射线之间的局部相关性来提高基于nerf的方法的表示能力。

具体而言，该框架由视图一致编码器和视图一致解码器两部分组成，如图6所示。

![image](https://user-images.githubusercontent.com/48575896/227128159-c832bd4f-732b-4960-95cb-2dec60c069b0.png)

编码器用于在较低分辨率空间中有效地编码场景的地理度量属性，形成中间射线特征和几何信息(即估计深度)输入到解码器。

解码器能够通过在高分辨率(全尺度)观测中集成深度调制卷积学习的几何感知局部模式来恢复高频细节。

我们进一步引入了一种基于补丁的射线采样策略，以取代nerf中的随机采样，允许编码器和解码器与面向感知的损失(即adversarial loss and perceptual loss)联合训练, 以补充传统的像素级MSE的损失。

更重要的是，这种联合训练有助于编码器中的几何建模与解码器中的局部上下文学习相协调，实现对精细细节的视图一致性增强。

在4K超高分辨率具有挑战性的场景下进行的经验比较和消融研究在定量和定性上都证明了所提出的框架的有效性。

我们进一步验证了该方法在不同基础架构上的通用性，以及在标准1K分辨率设置下提高视觉质量的效果。

## Method
我们首先回顾了基于nerf的体积绘制方法，并讨论了建模和渲染超高分辨率场景的局限性。

然后，我们详细介绍了我们的NeRF-4K框架，并在下一节中介绍了带有损失函数的训练策略。

### Volumetric Rendering
NeRF通过学习一个连续映射函数F利用3D点位置x∈R3和观看方向d∈R3来估计的颜色c∈R3和体积密度σ∈r，从而实现真实感视图合成，即:![image](https://user-images.githubusercontent.com/48575896/227132297-3d0f29bd-c3d2-4adf-bbb6-ea815b6e6817.png)

为了渲染给定相机姿态的图像，通过Max[24]讨论的数值求积来估计从相机中心o到像素的相机射线r = o + td的期望颜色![image](https://user-images.githubusercontent.com/48575896/227133098-24571e60-da43-440c-9f67-35a23296d68f.png)，方法是沿着射线采样一组点，并将它们的颜色积分来近似于体积渲染积分，

![image](https://user-images.githubusercontent.com/48575896/227133170-93ea2131-9b0b-469f-9213-49db331526cf.png)

其中，αi表示在i点的射线终止概率，δi = ti+1−ti表示两个adja分点之间的距离，ti表示到达i点时的累计透射率。

映射函数Φ被实例化为一个多层感知器(MLP)。

给定已知姿态的图像训练集，通过最小化预测像素颜色与真实颜色之间的均方误差(MSE)来训练模型。

![image](https://user-images.githubusercontent.com/48575896/227135845-00813f06-f861-40af-977c-3ddb4cadd73c.png)

式中 R 为每个最小批中随机抽样的射线集。

每个点的优化是根据它在不同视点光线中的投影来进行的。

有人提出了一些变体，将学习与显式结构集成在一起[23,29,36,46]，而不是单一的大型神经网络。

通过将密度预测限制在位置的使用上，NeRF族可以学习几何(即多视图)一致的表示，并通过学习位置和观看方向来实现依赖于视图的渲染结果。

### Limitation
所有这些方法尽管架构不同，但都是执行像素级别的方法。

射线(或像素)在训练和推断过程中是独立处理的。

射线集的基数随着图像分辨率的增加呈二次增长.

对于4K超高分辨率的图像，通常有超过800万像素，它们呈现更丰富的细节，每个像素都比低分辨率图像更自然地体现了场景的整体内容

如果直接使用这种像素级训练机制对具有极高高分辨率输入的场景建模，这些方法可能会在保留微妙细节方面存在不足的表示能力，即使增加了模型容量(如实验比较5.4所示)，这可能会恶化具有巨大MLP的冗长推理问题或使用具有增加体积维度的体素网格结构带来的相当大的存储成本。

### Overall Framework
为了在超高分辨率下扩展传统的NeRF方法以实现高质量的渲染，一个简单的解决方案是首先训练NeRF模型来渲染下采样的输出，然后在每个视图上训练参数化的超分辨率来上采样这些输出到全尺寸。

然而，这样的解决方案会导致跨视点渲染不一致的伪影，因为在超分辨率阶段捕获的局部模式缺乏几何一致性的正则化(如5.5中联合训练的消融研究所示)。

我们开发了一个简单而有效的NeRF-4K框架，建立在基于nerf的体传输方法上。

我们首先通过使用视图一致性En- coder(简称VC-Encoder)模块在低分辨率空间中编码几何信息，然后通过视图一致性解码器(简称VC-Decoder)模块在高分辨率空间中恢复细微细节。

该方法旨在通过积分提高基于nerf的模型在高频细节恢复方面的表征能力在观察中学习到的3d感知局部特征。

我们进一步实证证明，该框架能够在标准分辨率(即1K)下提高传统NeRF方法的视觉质量，其中编码器和解码器在具有相同分辨率的空间中学习(如5.5中使用射线特征局部相关性增强的消融研究所示)。

### View Consistent Encoder
我们实现VC-Encoder基于DVGO[36]中定义的公式，其中基于体素网格的表示被学习来显式编码几何结构，

![image](https://user-images.githubusercontent.com/48575896/227151636-8362492b-12ff-4927-b296-bd6d83c6e381.png)

其中Nc为密度的通道维数(Nc =1)色彩模态，分别。

对于每个采样点，密度通过配备softplus激活函数的三线性插值估计，即:![image](https://user-images.githubusercontent.com/48575896/227152436-a258e7d4-186c-4828-8586-5b41102a9482.png)

颜色是用浅MLP估计的，

![image](https://user-images.githubusercontent.com/48575896/227152503-30320d1a-37a4-4467-9cc7-7420dc78bf82.png)

其中gθ(·)提取颜色信息的体积特征，fRGB表示从特征到RGB图像的映射(具有一层或多层)。

我们将![image](https://user-images.githubusercontent.com/48575896/227157048-3543daaf-6d65-462f-8f7e-af4e61b8bc1a.png)作为VC-Encoder，输出![image](https://user-images.githubusercontent.com/48575896/227157096-a297bc52-6553-4103-92d4-786b24624ddd.png)为观察方向为d的点X的体积特征，该特征中嵌入了几何信息。

在这方面，我们可以通过将沿着射线r的采样点的特征累加得到每个射线(或像素)的描述符，如Eqn.1所示，

![image](https://user-images.githubusercontent.com/48575896/227157368-d33348ae-14a4-49ad-97d4-132af791b88d.png)

假设空间维度为H ' ×W '，形成的特征图Fen∈RC ' ×H ' ×W '输入VC-Decoder进行精细细节的高保真重构。

### View Consistent Decoder
为了更好地使用VC-Encoder embedded的几何属性，我们也生成一个深度图M∈RH0 ×W0通过估计每条射线r沿摄像机轴的深度，

![image](https://user-images.githubusercontent.com/48575896/227168825-b0c7e27b-e036-4e04-80c0-0005b4a1f744.png)

式中，ti为采样点i到摄像机中心的距离，如Eqn.1。

估计的深度图为低估场景的3D结构提供了强有力的指导，例如，图像平面上附近的像素在原始3D空间中可能很远。

VC-Decoder的构建方式如：
![image](https://user-images.githubusercontent.com/48575896/227878428-f6dade40-6a9f-4637-92d2-78b3d52331f7.png)作为输入，通过卷积神经网络Ψ:(Fen,M) → P实现更高空间维度H × W的视图合成P，其中![image](https://user-images.githubusercontent.com/48575896/227878980-f141760b-6891-40c6-a8a6-fbcd0d8b65e6.png)

![image](https://user-images.githubusercontent.com/48575896/227879274-77e48cd8-8373-4627-abf8-c8175960a44d.png) 式中s为上采样尺度。

该网络是通过堆叠几个卷积块(既没有非参数归一化也没有下采样操作)，并穿插上采样操作来构建的。特别地，我们不是简单地将特征Fen和深度图M连接在一起，而是将深度信号分开，通过学习变换将其注入到每个块中来调制块的激活。

形式上，假设![image](https://user-images.githubusercontent.com/48575896/227879590-28e7c1d8-4a7b-4f84-bdc9-11cdf9b8f46a.png)表示通道维度为Ck的中间块的激活。

深度图M经过变换(如1 × 1卷积)得到具有相同信道维数Ck的预测标度和偏置值，用于对F k进行调制:

![image](https://user-images.githubusercontent.com/48575896/227879714-ace3bfc5-f503-4059-bce3-6d226a2dcdf5.png)

其中![image](https://user-images.githubusercontent.com/48575896/227879826-503938d4-4fbe-48f0-ac80-49cbf586195c.png)表示元素乘积，I和j表示空间位置。关于网络架构的更详细的描述可以在实现部分和补充材料中找到。

## Conclusion
在本文中，我们探索了NeRF对精细细节建模的能力，并提出了一个新的框架，以提高其在高分辨率场景中恢复视图一致的精细细节的表征能力。

一个引入编码器-解码器模块对来实现在低分辨率空间中有效地对几何属性进行建模，实现了视图一致性增强利用局部相关捕获满量程空间在几何感知特征之间。

该框架的基于补丁的采样训练使得该方法能够将基于感知的正则化的监督整合到像素级比较之外。

在真实数据集上的实验验证了我们的框架能够在保持视图一致性的情况下实现对细节的高保真渲染。

我们希望将该框架纳入动态场景建模以及神经渲染任务的影响作为未来的发展方向。
