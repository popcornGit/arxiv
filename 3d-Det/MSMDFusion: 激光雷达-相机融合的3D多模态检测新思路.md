# MSMDFusion: 激光雷达-相机融合的3D多模态检测新思路
## 介绍
融合激光雷达和相机信息对于在自动驾驶系统中实现准确可靠的3D目标检测至关重要，由于难以将来自两种截然不同的模态的多粒度几何和语义特征结合起来，这是一个很大挑战。

最近的方法旨在通过将2D相机图像中的提升点（称为“seed”）引入3D空间来探索相机特征的语义密度，然后通过跨模态交互或融合技术来结合2D语义。

然而，当将点提升到3D空间时，这些方法中的深度信息研究不足，因此2D语义不能与3D点可靠地融合。

此外，这些多模态融合策略（被实现为串联或关注）要么不能有效地融合2D和3D信息，要么不能在体素空间中执行细粒度交互。

为此，本文提出了一个新的框架，该框架更好地利用了深度信息和激光雷达与相机之间的细粒度交叉模式交互，该框架由两个重要组件组成。

首先，使用具有深度感知设计的多深度非投影（MDU）方法来增强每个交互级别的提升点的深度质量。

其次，应用门控模态感知卷积（GMA卷积）块以细粒度方式调制与相机模态相关的体素，然后将多模态特征聚合到统一空间中。

它们一起为检测头提供了来自激光雷达和相机的更全面的功能。

在nuScenes测试基准上，提出的方法（简称MSMDFusion）以71.5%的mAP和74.0%的NDS实现了最先进的3D目标检测结果，并以74.0%的AMOTA实现了强大的跟踪结果，而无需使用测试时间增强和集成技术

代码可在：https://github.com/SxJyJay/MSMDFusion上获得！

![25505075eda5ae814e90968e5a7ab89f](https://user-images.githubusercontent.com/48575896/227919867-baf04538-de86-4820-8841-f552fefa0862.png)

检测3D目标被视为自动驾驶的基本任务，针对强大的环境感知，自动驾驶车辆上一般广泛配备了激光雷达和camera，因为它们可以提供补充信息。

以点云为特征，激光雷达可以捕捉精确的空间信息，而相机包含丰富的语义和图像上下文。

因此，开发能够享受两个传感器好处的多模态检测器是有希望的。

这样的想法推动了一系列最新研究的出现[1，3，12，13，16，18，22，23，28，29]。早期的工作[1，3，11，12，20，22，23，29]通过将3D LiDAR点（或从其生成的区域建议）投影到2D图像平面上以收集有用的2D语义来执行LiDAR相机融合。

然而，这种范例受到多模态传感器的信号密度失配的影响，由于LiDAR点比相机像素稀疏得多，这种投影方式将不可避免地浪费语义丰富的2D特征。

最近，出现了另一种用于激光雷达相机融合的范例，这些方法不是通过3D查询来收集2D语义特征，而是首先估计像素的深度，然后使用它们对3D世界的语义（在本文中，将这些像素和相应的提升点称为“种子”和“虚拟点”）与真实的3D点云融合。

两种与BEVFusion同名的方法将每个图像特征像素视为seed，并在BEV空间中生成虚拟点。

MVP和VFF从前景区域采样像素并将其提升到体素空间，得益于密集的虚拟点，这种范式不仅保持了图像中的语义一致性，而且补充了稀疏LiDAR点云的几何线索。

尽管已经取得了重大改进，但这方面的现有方法存在两个主要问题，这阻碍了从虚拟点中获益。

首先，深度作为虚拟点质量的关键，在生成虚拟点时研究不足。

一方面，深度通过透视投影直接确定seed在3D空间中的空间位置，从而显著影响3D目标检测结果。

另一方面，深度还可以通过在描述目标时提供颜色不敏感的线索来增强虚拟点所承载的语义，因为将RGB信息与深度引导相结合，可以关联具有相似深度的相机像素，并使它们能够在被提升为虚拟点时共同有助于捕获与实例相关的语义。

现有的多模态检测器主要关注LiDAR点与相机虚拟点的交互，而忽略了seed深度在生成虚拟点中的重要性。

虚拟点和未压缩空间（例如，体素空间）中的3D点之间的细粒度交叉模态交互是至关重要的.

由于深度不完善和固有的模态差距，生成的虚拟点在几何和语义上与真实的LiDAR点不一致。

因此，为了从语义丰富的虚拟点中获益，有必要在真实LiDAR点的指导下以细粒度和可控的方式从虚拟点中自适应地选择有用信息。

然而，这种跨模式交互受到点云数据的大量和非结构化性质带来的密集内存和计算成本的限制。

或者，现有的方法将多模态信息与简单的连接或在体素空间中添加operation相结合，或者在压缩的BEV空间中执行交叉关注。

为了释放虚拟点的潜力并解决现有方法的缺点，本文提出了一种多尺度融合框架，称为MSMDFusion，在每个尺度内，有两种关键的新颖设计，即多深度非投影（MDU）和门控模态感知卷积（GMA卷积）。

如图1所示，MDU主要用于提高生成的虚拟点的几何精度和语义丰富性。

当将2D seed从图像提升到3D空间时，在参考邻域内探索多个深度以生成具有更可靠深度的虚拟点。

接下来，将相机特征和深度相结合以产生深度感知特征，作为更强的2D语义来装饰这些虚拟点。

GMA Conv将真实的LiDAR点和生成的虚拟点作为输入，并以先选择后聚合的方式执行细粒度交互。

首先在参考LiDAR体素的指导下从相机体素特征中自适应地选择有用的信息，然后将其分组稀疏卷积进行聚合，以实现充分的多模态交互。

作者还特别采用了体素二次采样策略，以在实施GMA-Conv时有效地获得可靠的激光雷达参考。

最后，利用来自多个尺度的多模态体素特征，进一步将它们与跨尺度的级联连接相关联，以聚合多粒度信息。

利用上述设计，封装在虚拟点中的相机语义与LiDAR点一致地结合，从而提供用于增强3D对象检测的更强的多模态特征表示。

如表3所示，与两种BEVFusion方法相比，生成的虚拟点减少了100倍，本文的MSMDFusion仍然可以实现最先进的性能！

![5bc364db2c7dabc41dafd5598fb13062](https://user-images.githubusercontent.com/48575896/227925388-182454eb-f117-42e7-8cf9-abbf8eb5f9a0.png)

### 本文方法
MSMDFusion的总体视图如图2所示，给定LiDAR点云和相应的多视图相机图像作为输入，MSMDFusion首先从体素空间中的两种模态中提取多尺度特征。

然后，在多尺度体素空间内执行激光雷达相机交互，以适当地组合来自两种模态的多粒度信息。



![31844be226ae56a8f30654042d1e282c](https://user-images.githubusercontent.com/48575896/228201468-4b58f4b2-ad75-4d1b-827f-11f477c5d5a1.png)
