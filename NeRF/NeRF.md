# NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

## 摘要
我们提出了一种方法，通过使用稀疏的输入视图集优化underlying continuous volumetric scene function函数，实现了最先进的结果，用于合成复杂场景的新视图。

我们的算法使用全连接(非卷积)深度网络表示一个场景，其输入是一个连续的5D坐标(空间位置(x, y, z)和观看方向(θ, φ))，其输出是该空间位置的volume density和view-dependent emitted radiance。

我们通过沿着相机光线querying5D坐标来合成视图，并使用经典的体渲染技术将输出的颜色和密度投影到图像中。

因为体素渲染是自然可微的，所以优化我们的表示所需要的唯一输入是一组具有已知相机位姿的图像。

我们描述了如何有效地优化神经辐射场，以渲染具有复杂几何和外观的场景的逼真的新视图，并演示了优于先前在神经渲染和视图合成方面的工作的结果。

视图合成结果最好作为视频观看，因此我们敦促读者查看我们的补充视频，以便进行令人信服的比较。

## 介绍
  在这项工作中，我们以一种新的方式解决了长期存在的视图合成问题，直接优化连续5D场景表示的参数，以最大限度地减少渲染一组捕获图像的误差。

  我们将静态场景表示为一个连续的5D函数，它输出空间中每个点(x, y, z)在每个方向(θ， φ)上发射的辐射度(radiance emitted)，以及每个点上的密度，就像一个微分不透明度，通过光线经过点(x, y, z)来控制射线累积的辐射度。

我们的方法优化了一个深度全连接神经网络，没有任何卷积层(通常称为多层感知器或MLP)，通过从单个5D坐标(x, y, z， θ， φ)回归到单个体积密度和依赖于视图的RGB颜色来表示这个函数。

为了从特定的视点渲染神经辐射场(NeRF):

1)让摄像机光线穿过场景，生成一组采样的3D点

2)利用上诉3D点及其对应点二维观看方向作为神经网络的输入，以产生一组颜色和密度的输出，

3)使用经典的体素渲染技术将这些颜色和密度累积到2D图像中。

因为这个过程是自然可微的，我们可以使用梯度下降来优化这个模型，通过最小化每个观察到的图像和我们的表示所呈现的相应视图之间的误差。

通过将高体素密度和精确的颜色分配到包含真实底层场景内容的位置，最小化跨多个视图的这种误差，鼓励网络预测场景的一致模型。

![image](https://user-images.githubusercontent.com/48575896/226281916-bd1ed524-08e5-4d9e-b248-475d5ea34849.png)

  我们发现，优化复杂场景的神经辐射场representation的经典实现不会收敛到足够高的分辨率表示，并且在每个相机光线所需的样本数量方面效率低下。

我们通过使用位置编码转换输入5D坐标来解决这些问题，该编码使MLP能够表示更高频率的函数，并且我们提出了一种分层采样过程，以减少对这种高频场景表示进行充分采样所需的查询数量。

  我们的方法继承了体素表示的优点:两者都可以表示复杂的现实世界的几何和外观，并且非常适合使用投影图像进行基于梯度的优化。

至关重要的是，我们的方法克服了离散体素网格在高分辨率建模复杂场景时令人望而却步的存储成本。

综上所述，我们的技术贡献有:

  一种将具有复杂几何和材料的连续场景表示为5D神经辐射场的方法，参数化为基本MLP网络。
  
  一个基于经典体素渲染技术的可微分渲染过程，我们用它来优化标准RGB图像的representations。这包括一个分层抽样策略，将MLP的容量分配给具有可见场景内容的空间。
  
  将每个输入5D坐标映射到高维空间的位置编码，使我们能够成功优化神经辐射场来表示高频场景内容。
  
我们证明，我们得到的神经辐射场方法在定量和定性上优于最先进的视图合成方法，包括将神经3D表示适合场景的工作，以及训练深度卷积网络来预测采样的体积表示的工作。

据我们所知，本文提出了第一个连续神经场景表示，能够从自然环境中捕获的RGB图像中渲染真实物体和场景的高分辨率逼真新视图。

## Neural Radiance Field Scene Representation
我们将连续场景表示为5D向量值函数，其输入是3D位置x = (x, y, z)和2D观看方向(θ， φ)，其输出是发射颜色c = (r, g, b)和体积密度σ。

在实践中，我们用三维笛卡尔单位向量d来表示方向。

我们用MLP网络FΘ:(x, d)→(c， σ)近似这种连续5D场景表示，并优化其权重Θ以将每个输入5D坐标映射到其相应的体积密度和定向发射颜色。

通过限制网络预测体积密度σ仅作为位置x的函数，同时允许RGB颜色c作为位置和观看方向的函数来预测，我们鼓励representation是多视图一致的。

为了实现这一点，MLP FΘ首先处理输入三维坐标x与8个全连接层(使用ReLU激活和每层256通道)，并输出σ和256维特征向量。

然后，该特征向量与摄像机光线的观看方向连接，并传递到一个额外的全连接层(使用ReLU激活和128个通道)，该层输出依赖于视图的RGB颜色。

图3是我们的方法如何使用输入观察方向来表示非兰伯效应的例子。如图4所示，一个没有视图依赖的训练模型(只有x作为输入)很难表示镜面。

## Volume Rendering with Radiance Fields
我们的5D神经辐射场表示空间中任意点的体素密度和定向发射辐射的场景。

我们使用经典体渲染[16]的原理来渲染任何穿过场景的光线的颜色。

体素密度σ(x)可以解释为射线在x位置上终止于无限小粒子的微分概率。

期望的颜色具有远近界tn和tf的相机射线r(t) = o + td的C(r)为:

![image](https://user-images.githubusercontent.com/48575896/226292231-a0fc6447-a8b3-4307-b186-d37323781d4f.png)

函数T(t)表示沿射线从tn到T的累计透过率，即，射线从tn到T而不撞击任何其他粒子的概率。

从我们的连续神经辐射场渲染视图需要估计通过所需虚拟相机的每个像素跟踪的相机光线的积分C(r)。

我们用积分法对这个连续积分进行了数值估计。

确定性正交，通常用于呈现离散体素网格，将有效地限制我们表示的分辨率，因为MLP只会在固定的离散位置集上查询。

相反，我们使用分层抽样方法，将[tn, tf]划分为N个均匀间隔的容器，然后从每个容器中均匀随机地抽取一个样本:

![image](https://user-images.githubusercontent.com/48575896/226298782-9cbb99f8-0a32-4123-b823-c323f83f8c6f.png)

尽管我们使用离散的样本集来估计积分，分层抽样使我们能够表示连续的场景表示，因为这个抽样方法导致MLP在优化过程中在连续的位置被评估。

我们使用这些样本，用Max[26]在体素渲染综述中讨论的正交规则来估计C(r):

![image](https://user-images.githubusercontent.com/48575896/226298446-4e5f81e4-3a48-4b57-b09a-e41b44a7cba5.png)

其中δi = ti+1 − ti为相邻样本之间的距离。

这个函数用于计算![image](https://user-images.githubusercontent.com/48575896/226299633-66520530-b0b6-470c-87b9-1483d2456433.png)从(ci， σi)值是平凡可微的，并简化为传统的α值 αi = 1 - exp(−σiδi)。

## Optimizing a Neural Radiance Field
在前一节中，我们已经描述了将场景建模为神经辐射场并从该表示中渲染新视图所必需的核心组件。

然而，我们观察到这些组件不足以达到最先进的质量，如第6.4节所述)。

我们引入了两个改进来表示高分辨率的复杂场景。

第一个是输入坐标的位置编码，帮助MLP表示高频函数，第二个是分层抽样过程，允许我们有效地对高频表示进行抽样。

### Positional encoding
尽管神经网络是通用函数逼近器[14]，但我们发现，让网络FΘ直接操作xyzθφ输入坐标会导致渲染图在表示颜色和几何形状的高频变化方面表现不佳。

这与Rahaman等人最近的工作是一致的，该工作表明深度网络倾向于学习低频函数。

他们还表明，在将输入传递给网络之前，使用高频函数将输入映射到高维空间，可以更好地拟合包含高频变化的数据。

我们在神经场景表示的上下文中利用这些发现，并表明将FΘ重新定义为两个函数的组合![image](https://user-images.githubusercontent.com/48575896/226304449-1e0674da-343f-4f99-9349-539544e59d74.png)，显著提高了性能(图4和表2)。

这里γ是从![image](https://user-images.githubusercontent.com/48575896/226307048-2562767c-1b5c-43d3-a9f1-d2aa533355ac.png)到高维空间![image](https://user-images.githubusercontent.com/48575896/226307159-1103bfdb-235f-4251-984a-ad557ce28c83.png)的映射和![image](https://user-images.githubusercontent.com/48575896/226307278-475d420a-fb9a-4d0c-9471-f179f78df106.png)仍然只是一个普通的MLP。形式上，我们使用的编码函数是:

![image](https://user-images.githubusercontent.com/48575896/226304783-d2cb87e0-2dea-44b5-8a2e-7413b49416aa.png)

这个函数γ(·)分别应用于x中的三个坐标值(归一化为位于[- 1,1])和笛卡尔观看方向单位向量d的三个组成部分(构造为[−1,1])。

在我们的实验中，我们设置γ(x)为L = 10， γ(d)为L = 4。

在流行的Transformer体系结构[47]中也使用了类似的映射，它被称为位置编码。

但是，transformer将其用于不同的目标，即提供序列中令牌的离散位置，作为不包含任何顺序概念的体系结构的输入。

相比之下，我们使用这些函数将连续输入坐标映射到高维空间，以使我们的MLP更容易近似更高频率的函数。

基于投影[51]的三维蛋白质结构建模相关问题的并行工作也利用了类似的输入坐标映射。

###  Hierarchical volume sampling
我们沿着每条摄像机光线在N个查询点上密集评估神经辐射场网络的渲染策略是低效的:对渲染图像没有贡献的自由空间和闭塞区域仍然被重复采样.

我们从体素渲染[20]的早期工作中获得灵感，并提出了一种分层表示，通过按比例分配样本来提高渲染效率，从而达到最终渲染的预期效果。

我们不是只用一个网络来表示场景，而是同时优化两个网络:一个“粗”网络和一个“细”网络。

我们首先使用分层抽样对一组Nc位置进行抽样，并评估这些位置上的“粗”网络，如Eqns. 2和3所述。

给定这个“粗”网络的输出，然后我们沿着每条射线对点进行更明智的采样，其中样本偏向于体积的相关部分。

为此，我们首先从粗网络![image](https://user-images.githubusercontent.com/48575896/226310168-62026e7b-77e2-421a-8b05-0bc724ac3203.png)中重写alpha合成颜色Eqn. 3作为所有采样颜色ci沿射线的加权和:

![image](https://user-images.githubusercontent.com/48575896/226310276-973b505d-2192-4bce-80d0-d3e4ce06fc41.png)

将这些权重归一化为:![image](https://user-images.githubusercontent.com/48575896/226311632-9941a37d-93b1-4393-b93b-7b7a244d4833.png)产生一个分段常数沿射线方向。

我们使用反变换采样从这个分布中采样第二组Nf位置，在第一组和第二组样本的并集处评估我们的“精细”网络，并计算光线的最终渲染颜色![image](https://user-images.githubusercontent.com/48575896/226311961-d971c83b-9ace-4510-8b7d-5ce866ab23ea.png)使用Eqn. 3，但使用所有Nc+Nf样本。

这个过程将更多的样本分配到我们期望包含可见内容的区域。

这解决了与重要性抽样类似的目标，但我们使用抽样值作为整个积分域的非均匀离散化，而不是将每个样本作为整个积分的独立概率估计。

###  Implementation details
我们为每个场景优化了一个单独的神经连续体表示网络。

这只需要一个采集的场景RGB图像数据集、相应的相机姿态和内参以及场景边界(我们使用地面真实相机姿态、内在参数和合成数据边界，并使用COLMAP结构-来自运动包[39]来估计真实数据的这些参数)。

在每次优化迭代中，我们从数据集中所有像素的集合中随机抽取一批相机光线，然后按照第5.2节所述的分层抽样，从粗网络中查询Nc样本，从细网络中查询Nc + Nf样本。

然后，我们使用第4节中描述的体渲染程序来渲染来自两组样本的每条射线的颜色。

我们的损失仅仅是渲染和真实像素颜色之间的总平方误差，对于粗渲染和精细渲染:

![image](https://user-images.githubusercontent.com/48575896/226313485-99a0e8dd-7a70-4ee6-8ab3-5a190e8487e1.png)

R是每一批射线的集合![image](https://user-images.githubusercontent.com/48575896/226313735-65142ab4-4db6-470b-9268-4354110a582e.png)分别为射线r的ground truth，粗体积预测，细体积预测RGB颜色。

请注意，即使最终的渲染来自![image](https://user-images.githubusercontent.com/48575896/226313900-01b910fb-e1e6-43ba-b602-2c59488898d2.png)，我们还最小化了![image](https://user-images.githubusercontent.com/48575896/226313961-6f2e5055-96d9-4fe2-bbea-c6ef26edc204.png)的损失，使得粗网络中的权重分布可以用来在细网络中分配样本。

在我们的实验中，我们使用了4096条射线的批量大小，每条射线在粗体中Nc = 64坐标采样，在细体中Nf = 128额外坐标采样。
